# -*- coding: utf-8 -*-
"""PCE2-24/04/2022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fBBFvKiP205q5snMg0v9UUBRs7BmnuqA
"""

# Commented out IPython magic to ensure Python compatibility.
#!pip install nltk
#!pip install spacy
#!pip install markovify
#!pip install -m spacy download en
#!python -m spacy download es_core_news_sm
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from bs4 import BeautifulSoup
import requests
import pandas as pd
import numpy as np
import re
import es_core_news_sm
# %matplotlib inline
from wordcloud import WordCloud
import markovify
from nltk.util import ngrams

nlp = es_core_news_sm.load()
np=10 #aqui colocamos el numero de paginas del link que queremos que recorra nuestro script
#declaramos variables que usaremos luego
titulares = list()
fechas = list()
fakenews = list()
#en la siguiente parte del codigo hacemos el web scraping
for i in (range(0,np+1)) :
    link = ('https://www.bbc.com/mundo/topics/c2lej05epw5t/page/'+str(i))
    page = requests.get(link)
    soup = BeautifulSoup(page.content, 'html.parser')     
    ti =  soup.find_all('span', class_= 'lx-stream-post__header-text') 
    fe =  soup.find_all('span', class_= 'qa-post-auto-meta') 
    for j in ti:
        titulares.append(j.text)
    for j in fe:
        fechas.append(j.text)
#creamos el diccionario con fechas y titulares y usamos la libreria pandas para volverlo un csv
dict = {'Fecha': fechas, 'Titulares': titulares} 
df = pd.DataFrame(dict) 
df.to_csv('test.csv')
#escribo en la consola todos los titulares con sus fechas que extraimos de la pagina
for i in range(len(titulares)):
  print(fechas[i]+" "+titulares[i])
#utilizo esta funcion que limpia las oraciones de cosas que sobran como los signos de puntacion
def text_cleaner(text):
  text = re.sub(r'--', ' ', text)
  text = re.sub('[\[].*?[\]]', '', text)
  text = re.sub(r'(\b|\s+\-?|^\-?)(\d+|\d*\.\d+)\b','', text)
  text = ' '.join(text.split())
  return text
#utilizo la funcion en todos los titulares recopilados
for i in (range(len(titulares))):
   titulares[i] = text_cleaner(titulares[i])
#creo los titulos fakes
Palabrastitulares = " ".join(titulares)
Titulos = nlp(Palabrastitulares)
ti_sents = ' '.join([sent.text for sent in Titulos.sents if len(sent.text) > 1])
gfn = markovify.Text(titulares)
for i in range(30):
  fakenews.append((i+1,":",gfn.make_short_sentence(max_chars=150)))
  print(fakenews[i])
#hago el directorio para ver cuantas veces se repite cada palabra
inutiles = set(stopwords.words('spanish'))
dic={}
for i in titulares:
  k=i.split(" ")
  for j in k:
    if j.lower() not in inutiles: 
     if j in dic:
        dic[j]+=1
     else:
      dic[j]=1
dic= sorted(dic.items(), reverse=True) 
dic= {k: v for k, v in dic}
#creo la grafica de barras de palabras mas recurridas
x,y = list(dic.keys())[0:8],list(dic.values())[0:8]
plt.bar(x,y)
plt.ylabel=('palabras mas recurrentes')
plt.xlabel=('palabras')
plt.show()
#creo el mapa mental de las palabras mas repetiras 
wordcloud = WordCloud(width=500, height=500, max_words=40,background_color="white").generate(str(dic)) #Creamos el wordcloud
plt.figure() 
plt.imshow(wordcloud) 
plt.axis("off") 
plt.margins(x=0, y=0) 
plt.show()